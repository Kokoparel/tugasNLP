{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN/fV7ooI+W6aW8PbqjfL5Z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kokoparel/tugasNLP/blob/main/NLPkelompok5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K2kNcakc1faj",
        "outputId": "c38be14c-89ee-4b3e-b9f7-930d47a9e021"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: Sastrawi in /usr/local/lib/python3.12/dist-packages (1.0.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.3)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2025.11.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.3)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "pip install Sastrawi nltk pandas"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
        "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
        "\n",
        "# download resource tokenisasi NLTK (natural languange toolkit)\n",
        "nltk.download('punkt')\n",
        "\n",
        "#input teks\n",
        "text = \"\"\"\n",
        "Kecerdasan Buatan (AI) saat ini berkembang sgt pesat di Indonesia! Bnyk perusahaan yg mulai menggunakan teknologi ini utk efisiensi kerja.\n",
        "Namun, tantangan terbesarnya adlh ketersediaan data yg bersih...\n",
        "Data yg kotor bs bikin model AI jd kurang akurat, makanya proses preprocessing itu wajib bgt dilakukan sblm masuk ke tahap modeling.\n",
        "\"\"\"\n",
        "\n",
        "print(f\"teks asli:{text}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SBt0Axu14gVq",
        "outputId": "7f182f76-43d2-48e7-a0d5-b68e896edfac"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "teks asli:\n",
            "Kecerdasan Buatan (AI) saat ini berkembang sgt pesat di Indonesia! Bnyk perusahaan yg mulai menggunakan teknologi ini utk efisiensi kerja. \n",
            "Namun, tantangan terbesarnya adlh ketersediaan data yg bersih... \n",
            "Data yg kotor bs bikin model AI jd kurang akurat, makanya proses preprocessing itu wajib bgt dilakukan sblm masuk ke tahap modeling.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#sentence segmentation\n",
        "sentences = nltk.sent_tokenize(text)\n",
        "print(f\"{sentences}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jh4xxbFj5ijN",
        "outputId": "b619e333-b5b1-4620-d488-5b522b75ec98"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['\\nKecerdasan Buatan (AI) saat ini berkembang sgt pesat di Indonesia!', 'Bnyk perusahaan yg mulai menggunakan teknologi ini utk efisiensi kerja.', 'Namun, tantangan terbesarnya adlh ketersediaan data yg bersih... \\nData yg kotor bs bikin model AI jd kurang akurat, makanya proses preprocessing itu wajib bgt dilakukan sblm masuk ke tahap modeling.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#tokenisasi\n",
        "\n",
        "tokens = []\n",
        "for sentence in sentences:\n",
        "    # cleaning: ngapus tanda baca & angka, terus lowercase\n",
        "    clean_sent = re.sub(r\"[^\\w\\s]\", \"\", sentence).lower()\n",
        "\n",
        "    # tokenisasi\n",
        "    words = nltk.word_tokenize(clean_sent)\n",
        "    tokens.append(words)\n",
        "\n",
        "print(f\"{tokens}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pc_fWR2j6ZuP",
        "outputId": "43a5aca8-d318-4779-9a59-1c98b6639a3b"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['kecerdasan', 'buatan', 'ai', 'saat', 'ini', 'berkembang', 'sgt', 'pesat', 'di', 'indonesia'], ['bnyk', 'perusahaan', 'yg', 'mulai', 'menggunakan', 'teknologi', 'ini', 'utk', 'efisiensi', 'kerja'], ['namun', 'tantangan', 'terbesarnya', 'adlh', 'ketersediaan', 'data', 'yg', 'bersih', 'data', 'yg', 'kotor', 'bs', 'bikin', 'model', 'ai', 'jd', 'kurang', 'akurat', 'makanya', 'proses', 'preprocessing', 'itu', 'wajib', 'bgt', 'dilakukan', 'sblm', 'masuk', 'ke', 'tahap', 'modeling']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#normalisasi\n",
        "#normalisasi tetep butuh kamus manual karena library tidak tahu bahasa gaul yang spesifik\n",
        "norm_dict = {\n",
        "    'sgt': 'sangat',\n",
        "    'bnyk': 'banyak',\n",
        "    'yg': 'yang',\n",
        "    'utk': 'untuk',\n",
        "    'adlh': 'adalah',\n",
        "    'bs': 'bisa',\n",
        "    'jd': 'jadi',\n",
        "    'bgt': 'banget',\n",
        "    'sblm': 'sebelum'\n",
        "}\n",
        "\n",
        "normalized_tokens = []\n",
        "for sentence in tokens:\n",
        "    # ganti kata singkatan dengan kata baku\n",
        "    normalized_tokens.append([norm_dict.get(word, word) for word in sentence])\n",
        "\n",
        "print(f\"{normalized_tokens}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pQ_4x1IN7v1h",
        "outputId": "1a6dc5fc-67c7-48cf-906e-f1de736a4c49"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['kecerdasan', 'buatan', 'ai', 'saat', 'ini', 'berkembang', 'sangat', 'pesat', 'di', 'indonesia'], ['banyak', 'perusahaan', 'yang', 'mulai', 'menggunakan', 'teknologi', 'ini', 'untuk', 'efisiensi', 'kerja'], ['namun', 'tantangan', 'terbesarnya', 'adalah', 'ketersediaan', 'data', 'yang', 'bersih', 'data', 'yang', 'kotor', 'bisa', 'bikin', 'model', 'ai', 'jadi', 'kurang', 'akurat', 'makanya', 'proses', 'preprocessing', 'itu', 'wajib', 'banget', 'dilakukan', 'sebelum', 'masuk', 'ke', 'tahap', 'modeling']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#stopword removal sastrawi\n",
        "factory_stop = StopWordRemoverFactory()\n",
        "stopword_remover = factory_stop.create_stop_word_remover()\n",
        "\n",
        "filtered_tokens = []\n",
        "for sentence in normalized_tokens:\n",
        "    # gabut jadi kalimat dulu karena Sastrawi butuh input string\n",
        "    kalimat_utuh = \" \".join(sentence)\n",
        "    kalimat_stop = stopword_remover.remove(kalimat_utuh)\n",
        "\n",
        "    # dipecah lagi jadi list kata\n",
        "    filtered_tokens.append(kalimat_stop.split())\n",
        "\n",
        "print(f\"{filtered_tokens}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m_2pNIjX8Q9A",
        "outputId": "c87eb0b8-5b93-47df-8dae-91f2c32058d3"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['kecerdasan', 'buatan', 'ai', 'ini', 'berkembang', 'sangat', 'pesat', 'indonesia'], ['banyak', 'perusahaan', 'mulai', 'menggunakan', 'teknologi', 'untuk', 'efisiensi', 'kerja'], ['tantangan', 'terbesarnya', 'ketersediaan', 'data', 'bersih', 'data', 'kotor', 'bikin', 'model', 'ai', 'jadi', 'kurang', 'akurat', 'makanya', 'proses', 'preprocessing', 'wajib', 'banget', 'dilakukan', 'masuk', 'tahap', 'modeling']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#stemming sastrawi\n",
        "factory_stem = StemmerFactory()\n",
        "stemmer = factory_stem.create_stemmer()\n",
        "\n",
        "stemmed_tokens = []\n",
        "for sentence in filtered_tokens:\n",
        "    # lakukan stemming pada setiap kata\n",
        "    stemmed_tokens.append([stemmer.stem(word) for word in sentence])\n",
        "\n",
        "print(f\"{stemmed_tokens}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mJs2uIah93v_",
        "outputId": "24d6b8cb-381f-4fdf-c9ad-7d5653bdd45f"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['cerdas', 'buat', 'ai', 'ini', 'kembang', 'sangat', 'pesat', 'indonesia'], ['banyak', 'usaha', 'mulai', 'guna', 'teknologi', 'untuk', 'efisiensi', 'kerja'], ['tantang', 'besar', 'sedia', 'data', 'bersih', 'data', 'kotor', 'bikin', 'model', 'ai', 'jadi', 'kurang', 'akurat', 'makanya', 'proses', 'preprocessing', 'wajib', 'banget', 'laku', 'masuk', 'tahap', 'modeling']]\n"
          ]
        }
      ]
    }
  ]
}